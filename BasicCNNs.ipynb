{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision: Basic CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reza Mousavi, University of Virginia, mousavi@virginia.edu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from keras) (1.5.2)\n",
      "Requirement already satisfied: six in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from h5py->keras) (1.15.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for slicer: [Errno 2] No such file or directory: '/Users/reza/opt/anaconda3/lib/python3.8/site-packages/slicer-0.0.7.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_png { display: table-cell; text-align: center; vertical-align: middle; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.MathJax {font-size: 100%;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IPython display functions\n",
    "import IPython\n",
    "from IPython.display import display, HTML, SVG, Image\n",
    "\n",
    "# General Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-paper')\n",
    "plt.rcParams['figure.figsize'] = [10, 6] ## plot size\n",
    "plt.rcParams['axes.linewidth'] = 2.0 #set the value globally\n",
    "\n",
    "## notebook style and settings\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_png { display: table-cell; text-align: center; vertical-align: middle; } </style>\"))\n",
    "display(HTML(\"<style>.MathJax {font-size: 100%;}</style>\"))\n",
    "\n",
    "# For changing background color\n",
    "def set_background(color):\n",
    "    script = ( \"var cell = this.closest('.code_cell');\" \"var editor = cell.querySelector('.input_area');\" \"editor.style.background='{}';\" \"this.parentNode.removeChild(this)\" ).format(color)\n",
    "    display(HTML('<img src onerror=\"{}\">'.format(script)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Keras library for deep learning\n",
    "# https://keras.io/\n",
    "import keras\n",
    "from keras.datasets import mnist # MNIST Data set\n",
    "from keras.models import Sequential # Model building\n",
    "from keras.layers import * # Model layers\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use a well-known data set MNIST, which contains hand-written digits. Each image in the data has 784 pixels (28x28) and each value in each pixel ranges between 0-255 (single grayscale). For more information about MNIST, please visit: https://en.wikipedia.org/wiki/MNIST_database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set into training and testing set\n",
    "(trainImages, trainLabels), (testImages, testLabels) = mnist.load_data()\n",
    "\n",
    "# Save copies of the data. We will use this later on when we develop our deep learning architecture\n",
    "rawTrainingImages, rawTestingImages = np.array(trainImages), np.array(testImages)\n",
    "\n",
    "rawtestLabels = testLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward, we take a look at the first image in the trainImages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainImages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the image is a list of lists (i.e. a matrix) of numbers. We can use pyplot's imshow() to display this image:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb383df9b80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainImages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image in the training data should also have a label. Let's see what's the label for this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep Learning for Image Recognition\n",
    "## 2.1 Feed-forward Neural Networks\n",
    "Before diving into the more complex deep learning architectures, we will first try to recognize images with the most simple form of neural networks i.e feed-forward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ml4a.github.io/images/figures/mnist_2layers.png\">\n",
    "<hr>\n",
    "<center><b>Feed-forward Neural Network for MNIST Digit Recognition</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the feed-forward neural network to process images as data, we need to flatten the images first. This will convert each 28x28 image into a single vector of 784 values. We use the function image.ravel() to do this. Essentially, ravel() flattens a 2 dimensional list into a single list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = np.array([image.ravel().astype('float32') for image in list(trainImages)])\n",
    "testImages = np.array([image.ravel().astype('float32') for image in list(testImages)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the values inside pixels range between 0 and 255, we divide all of them by 255 to make them range between 0 and 1. This is called normalization. Neural Nets work better with normalized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = trainImages / 255.0\n",
    "testImages = testImages / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert class labels to one-hot encoded representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = keras.utils.to_categorical(trainLabels, 10)\n",
    "testLabels = keras.utils.to_categorical(testLabels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the transformed label of our first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we pre-processed our images such that they can be fed to a feed-forward neural network. Now, we use the following function to design our feed-forward neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeedforwardNeuralNetwork(trainImages, numLayers, numUnits, dropoutValue, numClasses):\n",
    "    # Create model object\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the first layer with dropout\n",
    "    model.add(Dense(numUnits, input_shape = trainImages.shape[1:]))\n",
    "    model.add(Dropout(dropoutValue))\n",
    "    \n",
    "    # Add more layers if numLayers > 1\n",
    "    while numLayers > 1:\n",
    "        model.add(Dense(numUnits))\n",
    "        model.add(Dropout(dropoutValue))\n",
    "        numLayers = numLayers - 1\n",
    "    \n",
    "    # Add prediction layer. Softmax activation function is used with the final prediction layer\n",
    "    model.add(Dense(numClasses, activation = 'softmax'))\n",
    "    \n",
    "    # Compile model. You can skip this line.\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the neural net, we need to specify the parameter values. Feel free to change them if you wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################################################\n",
    "# Things you can change                                                                                                                             \n",
    "#####################################################################################################################################################\n",
    "numLayers = 1 # Number of layers in the neural network\n",
    "numUnits = 128 # Number of units in each layer\n",
    "dropoutValue = 0.5 # Dropout probability\n",
    "numClasses = 10 # Don't change this value for MNIST since we have 10 classes i.e we are trying to recognize a digit among 10 digits. But for any other data set, this should be changed\n",
    "epochsToTrain = 10 # Epochs for training\n",
    "batchSize = 32 # How many images should a single batch contain\n",
    "#####################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go ahead and take a look at the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = createFeedforwardNeuralNetwork(trainImages, numLayers, numUnits, dropoutValue, numClasses)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go ahead and train the model using our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 642us/step - loss: 0.5853 - accuracy: 0.8214\n",
      "1875/1875 [==============================] - 1s 636us/step - loss: 0.3535 - accuracy: 0.9005\n",
      "1875/1875 [==============================] - 1s 663us/step - loss: 0.3406 - accuracy: 0.9040\n",
      "1875/1875 [==============================] - 1s 654us/step - loss: 0.3325 - accuracy: 0.9063\n",
      "1875/1875 [==============================] - 1s 637us/step - loss: 0.3257 - accuracy: 0.9092\n",
      "1875/1875 [==============================] - 1s 640us/step - loss: 0.3230 - accuracy: 0.9090\n",
      "1875/1875 [==============================] - 1s 642us/step - loss: 0.3174 - accuracy: 0.9113\n",
      "1875/1875 [==============================] - 1s 638us/step - loss: 0.3152 - accuracy: 0.9126\n",
      "1875/1875 [==============================] - 1s 643us/step - loss: 0.3153 - accuracy: 0.9112\n",
      "1875/1875 [==============================] - 1s 639us/step - loss: 0.3120 - accuracy: 0.9127\n",
      "\n",
      "Best test accuracy using feedforward neural networks is: 92.54\n"
     ]
    }
   ],
   "source": [
    "bestTestAccuracy = 0\n",
    "for epoch in range(epochsToTrain):\n",
    "    model.fit(trainImages, trainLabels, batch_size=batchSize, epochs=1, verbose=1)\n",
    "    testAccuracy = model.evaluate(testImages, testLabels, verbose=0)[1]\n",
    "    if testAccuracy > bestTestAccuracy:\n",
    "        bestTestAccuracy = testAccuracy\n",
    "print(\"\\nBest test accuracy using feedforward neural networks is: %.2f\" % (bestTestAccuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to solve the same problem using a CNN. Basically, we add convolutional and pooling layer(s) before the feed-forward layer(s):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\" width=\"800px\">\n",
    "<hr>\n",
    "<center><b>Convolutional Neural Networks for MNIST Digit Recognition</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following function to design our CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConvolutionalNeuralNetwork(trainImages, numLayers, numFilters, kernelSize, maxPooling, dropoutValue, numClasses):\n",
    "    # Create model object\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the first layer with dropout\n",
    "    model.add(Conv2D(numFilters, kernel_size=(kernelSize, kernelSize),\n",
    "                     activation='relu',\n",
    "                     input_shape=trainImages.shape[1:]))\n",
    "    model.add(MaxPooling2D(pool_size=(maxPooling, maxPooling)))\n",
    "    model.add(Dropout(dropoutValue))\n",
    "    \n",
    "    while numLayers > 1:\n",
    "        model.add(Conv2D(numFilters, kernel_size=(kernelSize, kernelSize),\n",
    "                     activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(maxPooling, maxPooling)))\n",
    "        model.add(Dropout(dropoutValue))\n",
    "        \n",
    "        numLayers = numLayers - 1\n",
    "        \n",
    "    # Convolutional layers are done, adding the remaining stuff\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(dropoutValue))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "\n",
    "    # Compile model. You can skip this line.\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.code_cell');var editor = cell.querySelector('.input_area');editor.style.background='#fce53a';this.parentNode.removeChild(this)\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_background('#fce53a')\n",
    "\n",
    "#####################################################################################################################################################\n",
    "# Things you can change                                                                                                                             \n",
    "#####################################################################################################################################################\n",
    "numLayers = 1 # Number of layers in the neural network\n",
    "numFilters = 32 # Number of units in each layer\n",
    "kernelSize = 5 # filter size of a single filter\n",
    "dropoutValue = 0.2 # Dropout probability\n",
    "maxPooling = 2 # Max pooling\n",
    "numClasses = 10 # Don't change this value for MNIST since we have 10 classes i.e we are trying to recognize a digit among 10 digits. But for any other data set, this should be changed\n",
    "epochsToTrain = 10 # Epochs for training\n",
    "batchSize = 32 # How many images should a single batch contain\n",
    "#####################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we saved the original data in rawTrainingImages and rawTestingImages. These image data are in 28 by 28 pixels. To be able to feed these images to the CNN, we need to add the channel (color) information to the images. So, we convert each 28 by 28 matrix, to a 28 by 28 by c matrix, where c is the number of channels. Given that all these images are in grayscale channel, we only have one channel. Hence, n = 1. Therefore, we need to convert the 28 by 28 matrices to 28 by 28 by 1 matrices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before adding the channel info:\n",
    "rawTrainingImages[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTrainingImages = rawTrainingImages.reshape(len(rawTrainingImages), 28, 28, 1)\n",
    "rawTestingImages = rawTestingImages.reshape(len(rawTestingImages), 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After adding the channel info:\n",
    "rawTrainingImages[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can go ahead and create the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 296,458\n",
      "Trainable params: 296,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = createConvolutionalNeuralNetwork(rawTrainingImages, numLayers, numFilters, kernelSize, maxPooling, dropoutValue, numClasses)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can go ahead and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 9s 5ms/step - loss: 1.8300 - accuracy: 0.6649\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1929 - accuracy: 0.9452\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1398 - accuracy: 0.9603\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1178 - accuracy: 0.9668\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1007 - accuracy: 0.9709\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0951 - accuracy: 0.9728\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0843 - accuracy: 0.9756\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0841 - accuracy: 0.9765\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0777 - accuracy: 0.9790\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0764 - accuracy: 0.9789\n",
      "\n",
      "Best test accuracy using Convolutional Neural Network is: 98.47\n"
     ]
    }
   ],
   "source": [
    "bestTestAccuracy = 0\n",
    "for epoch in range(epochsToTrain):\n",
    "    model.fit(rawTrainingImages, trainLabels, batch_size=batchSize, epochs=1, verbose=1)\n",
    "    testAccuracy = model.evaluate(rawTestingImages, testLabels, verbose=0)[1]\n",
    "    if testAccuracy > bestTestAccuracy:\n",
    "        bestTestAccuracy = testAccuracy\n",
    "print(\"\\nBest test accuracy using Convolutional Neural Network is: %.2f\" % (bestTestAccuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use our CNN to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = model.predict_classes(rawTestingImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to obtain the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(model.predict(rawTestingImages), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and check the prediction for the very first image in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted number is 7, and the actual number was 7.\n"
     ]
    }
   ],
   "source": [
    "print(\"The predicted number is {}, and the actual number was {}.\".format(predicted_classes[0],rawtestLabels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what the original image was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb3840ad4c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rawTestingImages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
